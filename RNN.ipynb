{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for Next-Bit Prediction\n",
    "\n",
    "There are many tutorials on Recurrent Neural Networks. This notebook shows that computations on RNNs can be conveniently factored according to the <b>refresh-extract</b> paradigm, shared by other primivites such as pseudorandom number generators. \n",
    "The basic (vanila) implementation is then evaluated on the task of predicting next bits for a Markov chain (a model used for studying outputs of random generators) and compared with best theoretical accuracy.\n",
    "\n",
    "\n",
    "The code is essentially equivalent to a network built out of $\\texttt{tf.contrib.rnn.BasicRNNCell}$ iterated through the $\\texttt{tf.nn.static_rnn}$ method.\n",
    "\n",
    "## Refresh-Extract Pattern\n",
    "\n",
    "RNNs computation can be factored in two steps:  \n",
    "<ul>\n",
    "    <li>$\\texttt{Refresh}$: refreshing the internal state with an input</li>\n",
    "    <li>$\\texttt{Extract}$: extracting the output from the current internal state</li>\n",
    "</ul>\n",
    "\n",
    "In this they are similar to cryptographic random number generators.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_{t-1},X_{t} & \\overset{refresh}{\\longrightarrow} S_t \\\\\n",
    "S_{t} & \\overset{extract}{\\longrightarrow} Y_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As for implementation details $\\texttt{Refresh}$ merges the input with the current state and tranforms it through a hidden layer, while $\\texttt{Extract}$ transforms the state into a distribution used to sample the output $Y_i$ (it is convenient to work with the distribution when training).\n",
    "\n",
    "A forward pass is a sequence of $T$ steps each executing \"refresh\" and then \"extract\" operation. It initializes the state in the beginning and provides $T$ inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "xavier = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "## Refreshing ##\n",
    "\n",
    "def refresh(state,new_input):\n",
    "    with tf.variable_scope('refresh',reuse=True):\n",
    "        W = tf.get_variable('W')\n",
    "        b = tf.get_variable('b')\n",
    "        return tf.nn.tanh(tf.matmul(tf.concat([state,new_input],axis=-1),W)+b)\n",
    "    \n",
    "## Extraction ##\n",
    "    \n",
    "def extract(state):\n",
    "    with tf.variable_scope('extract',reuse=True):\n",
    "        W = tf.get_variable('W')\n",
    "        b = tf.get_variable('b',shape=(dim_target_out,))\n",
    "        return tf.nn.tanh(tf.matmul(state,W)+b)\n",
    "    \n",
    "## Forward Pass ##\n",
    "    \n",
    "def eval_forward(state0,X_list):\n",
    "    states = []\n",
    "    outputs = []\n",
    "    state = state0\n",
    "    for x in X_list:\n",
    "        state = refresh(state,tf.to_float(x))\n",
    "        states.append(state)\n",
    "        output = extract(state)\n",
    "        outputs.append(output)\n",
    "    return outputs,states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "\n",
    "To train a RNN we want to maximize the likelihood of the output sequence $\\{Y_t\\}_t$.\n",
    "Denoting by $\\{\\hat{y}_t\\}_t$ the predicted distribution we can write the log-likelihood as \n",
    "\n",
    "$$\\sum_{t} \\log p(\\left. Y_t\\ \\right|\\ X_{ t}) = \\sum_{i}\\mathsf{OneHot}(Y_t)\\cdot \\log \\hat{y}_t$$\n",
    "\n",
    "\n",
    "The RNN equations contain information about all previous data points. When fitting the model we optimize a truncated $T$-step pass.\n",
    "\n",
    "$$\n",
    "S_{t-1},X_{t}\\quad \\underbrace{\\overset{Refresh-Extract}{\\longrightarrow\\ldots\\longrightarrow}}_{T-fold}\\quad S_{t+T-1},X_{t+T}\n",
    "$$\n",
    "\n",
    "which avoids issues with optimizing long chained expressions (vanishing/exploding gradients). \n",
    "\n",
    "More precisely, during the $i$-th step we optimize the loss (negative log-likelihood) of the data generated by the forward pass\n",
    "from time $i\\cdot T+1$ to time $(i+1)\\cdot T$. The very first state is initialized with zeros. The graph for $T=5$ is shown below (note it reflects the refresh-extract logic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./graph.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"./graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## Global params ##\n",
    "\n",
    "dim_target_out = 2 # target out-dimension \n",
    "num_step = 5 # backpropagation length \n",
    "dim_state = 10\n",
    "batch_size = 250\n",
    "\n",
    "## Placeholders ##\n",
    "\n",
    "X = tf.placeholder(dtype=tf.int32,shape=(batch_size,num_step),name='X')\n",
    "Y = tf.placeholder(dtype=tf.int32,shape=(batch_size,num_step),name='Y')\n",
    "\n",
    "## Rearrange inputs ##\n",
    "X_enc = tf.one_hot(X,dim_target_out,axis=-1,name='X-onehot') # shape = (batch_size,window,target_out)\n",
    "X_list = tf.unstack(X_enc,axis=1,name='X-list') # list of (batch_size,target_out) of length=window\n",
    "\n",
    "## Refreshing params ##\n",
    "\n",
    "with tf.variable_scope('refresh'):\n",
    "    W = tf.get_variable('W',shape=(dim_state+dim_target_out,dim_state),initializer=xavier)\n",
    "    b = tf.get_variable('b',shape=(dim_state,),initializer=tf.constant_initializer(0))\n",
    "\n",
    "## Extraction params ##\n",
    "with tf.variable_scope('extract'):\n",
    "    W = tf.get_variable('W',shape=(dim_state,dim_target_out),initializer=xavier)\n",
    "    b = tf.get_variable('b',shape=(dim_target_out,),initializer=tf.constant_initializer(0))\n",
    "    \n",
    "## State initialization ##\n",
    "\n",
    "state0 = tf.zeros(shape=(batch_size,dim_state),name='state0')\n",
    "    \n",
    "## Forward pass ##\n",
    "outputs,states = eval_forward(state0,X_list)\n",
    "state = states[-1]\n",
    "\n",
    "\n",
    "## Likelihood / accuracy ##\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    outputs_t = tf.stack(outputs,axis=1,name='outputs')\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y,logits=outputs_t))\n",
    "    acc = tf.reduce_mean(tf.to_float(tf.equal(tf.to_int64(Y),tf.argmax(outputs_t,-1))))\n",
    "    \n",
    "with tf.variable_scope('optimize'):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a Markov Chain\n",
    "\n",
    "To illustrate RNN in action, let's evaluate it on a specific Markov chain on bits, which introduces bias depending on the partity of last 3 bits. In the example below the best theoretical accuracy for predicting next bit is $0.55$. A simple RNN model gets easily to about $0.53$, getting even closer to the limit would likely need more complicated architecture - the network needs essentially to learn parity and that is known to be little tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias -0.0006999999999999784\n"
     ]
    }
   ],
   "source": [
    "## data generation ##\n",
    "\n",
    "from collections import deque\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "def sequence(n,w=2):\n",
    "\n",
    "    tmp = deque()\n",
    "    tmp.extend(0 for _ in range(w))\n",
    "\n",
    "    for i in range(n):\n",
    "        if sum(tmp)%2==1:\n",
    "            threshold = 0.55\n",
    "        else:\n",
    "            threshold = 0.45\n",
    "        out = int( np.random.rand() < threshold )\n",
    "        tmp.append(out)\n",
    "        tmp.popleft()\n",
    "        yield out\n",
    "\n",
    "print('Bias %s'% (sum(i for i in sequence(10000))/10000 - 0.5))\n",
    "\n",
    "n_batch = 20000\n",
    "data = sequence(n_batch*batch_size*num_step,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7056767344474792, Accuracy=0.5040000081062317\n",
      "Loss=0.6926051188301254, Accuracy=0.5149626385617804\n",
      "Loss=0.6922789198169108, Accuracy=0.5192911554163304\n",
      "Loss=0.6921216523357329, Accuracy=0.5202937694896423\n",
      "Loss=0.6920359474782555, Accuracy=0.5210699334692223\n",
      "Loss=0.6919847836830072, Accuracy=0.5218194770565082\n",
      "Loss=0.6919466833376205, Accuracy=0.5225002509894877\n",
      "Loss=0.6919065237964771, Accuracy=0.5232907309223628\n",
      "Loss=0.6918831282802678, Accuracy=0.5240205985846303\n",
      "Loss=0.6918596496662555, Accuracy=0.5247305866354453\n",
      "Loss=0.6918480030990413, Accuracy=0.5252302781145235\n",
      "Loss=0.6918263386058955, Accuracy=0.525697519574184\n",
      "Loss=0.6918035694663003, Accuracy=0.5260344316622444\n",
      "Loss=0.6917789162750309, Accuracy=0.5263206226790321\n",
      "Loss=0.6917551364798894, Accuracy=0.5266128430646808\n",
      "Loss=0.6917335394620657, Accuracy=0.5268571973144989\n",
      "Loss=0.6917238229811545, Accuracy=0.5269983137676801\n",
      "Loss=0.6917095350262754, Accuracy=0.5271617916445684\n",
      "Loss=0.6916987781531015, Accuracy=0.5272934181969283\n",
      "Loss=0.6916892238462807, Accuracy=0.5274141372696274\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #writer = tf.summary.FileWriter('./', sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    state_ = np.zeros(shape=(batch_size,dim_state))\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    for i in range(n_batch):\n",
    "        X_ = np.array(list(t for t in islice(data,batch_size*num_step)))\n",
    "        Y_ = np.concatenate([X_[1:],[np.random.randint(0,2)]],0)\n",
    "        X_ = X_.reshape(batch_size,num_step)\n",
    "        Y_ = Y_.reshape(batch_size,num_step)\n",
    "        loss_,state_,acc_,_=sess.run([loss,state,acc,optimizer],\n",
    "                           feed_dict={X:X_,Y:Y_,state0:state_})\n",
    "        total_acc += acc_\n",
    "        total_loss += loss_\n",
    "        if i%1000==0:\n",
    "            print('Loss=%s, Accuracy=%s'%(total_loss/(i+1),total_acc/(i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow API \n",
    "\n",
    "To use the tensorflow API one needs to replace the code pice in the paragraph \"## Forward pass ##\" by the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnn',initializer=xavier): # initialization important !\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(dim_state) \n",
    "    outputs, state = tf.nn.static_rnn(cell, X_list, initial_state=state0)\n",
    "outputs = [extract(o) for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4845379 1.0\n"
     ]
    }
   ],
   "source": [
    "# one-pass to visualize the computational graph\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./', sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    state_ = np.zeros((batch_size,dim_state))\n",
    "    train_loss,state_,acc_,_=sess.run([loss,state,acc,optimizer],\n",
    "                           feed_dict={X:np.ones(shape=(batch_size,num_step)),\n",
    "                                      Y:np.ones(shape=(batch_size,num_step)),\n",
    "                                      state0:state_})\n",
    "    state0 = state_\n",
    "    print(train_loss,acc_)\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
